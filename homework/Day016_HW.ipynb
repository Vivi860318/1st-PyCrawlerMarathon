# 作業：接下來定義一個爬蟲函數，這個函數的主要工作為：
# (1) 爬取當前關鍵字的解釋，並存入檔案(因為文章內容太多會佔滿整個頁面，所以存程檔案，方便後續檢視)
# (2) 萃取出當前關鍵字所引用的外部連結，當作新的查詢關鍵字
# (3) 把第(2)擷取到的關鍵字當作新的關鍵字，回到第(1)步，爬取新的關鍵字解釋。

import requests
import re
import os
from bs4 import BeautifulSoup

input_keyword = "網路爬蟲" #可以更換任何有興趣的關鍵字
utf8_url = repr(input_keyword.encode("utf-8")).upper() #utf8解碼並更換成大寫
utf8_url = utf8_url.replace("\\X", "%") #用%取代\x
print("%s: %s" % (input_keyword, utf8_url[2:-1:1])) #擷取中間編碼結果

#組成wiki關鍵字搜尋的網址格式
root_keyword_link = "/wiki/" + utf8_url[2:-1:1]
print(root_keyword_link)

def wikiArticle(key_word_link, key_word, recursive):
    if (recursive <= max_recursive_depth):
        print("遞迴圈[%d]-%s(%s)"%(recursive, key_word_link, key_word))
       
        headers = {
        "authority": "zh.wikipedia.org",
        "method": "GET",
        "path": "/wiki/" + root_keyword_link,
        "scheme": "https",
        "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "accept-encoding": "gzip, deflate, br",
        "accept-language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6,ja;q=0.5",
        "cache-control": "max-age=0",
        "cookie": "TBLkisOn=0; WMF-Last-Access=14-Jan-2020; WMF-Last-Access-Global=14-Jan-2020; GeoIP=TW:TPE:Taipei:25.05:121.53:v4",
        "referer": "https://www.google.com/",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "cross-site",
        "sec-fetch-user": "?1",
        "upgrade-insecure-requests": "1",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36"
        }

        url = "https://zh.wikipedia.org" + root_keyword_link 
        resp = requests.get(url, headers = headers)
        resp.encoding = "utf-8"

        html = BeautifulSoup(resp.text, "html5lib")
        content = html.find(name = "div", attrs = {"id": "mw-content-text"}).find_all(name="p")

        #建立wikiArticle
        output_dir = "WikiArticle"
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        output_file = "{outdir}/{outfile}.txt".format(
            outdir = output_dir, outfile = key_word.replace("/", "_")
        )
        if os.path.exists(output_file):
            os.remove(output_file)
        with open(output_file, "w", encoding = "utf-8") as fh:
            for paragraph in content:
                f = fh.write(paragraph.get_text())

        #part2 請參考範例2，萃取出本篇文章中所延伸引用的外部連結，並儲存在external_link_dict

        external_link_dict = dict({})
        for ext_link in content:
            a_tag = ext_link.find_all("a", href = re.compile("^(/wiki/)((?!;)\S)*$"))
            if len(a_tag) > 0:
                for link_string in a_tag:
                    a_link = link_string["href"]
                    a_keyword = link_string.get_text()
                    external_link_dict[a_link] = a_keyword
        print(external_link_dict)

        #part 3 : 將part 2 所收集的外部連結，當作新的關鍵字繼續跌代深入爬蟲
        if (len(external_link_dict) > 0):
            recursive = recursive + 1 #遞迴深度+1
            for k, v in external_link_dict.items():
                wikiArticle(k, v, recursive) #再次呼叫同樣的函數，執行同樣的流程
            
        
#執行
#定義爬取的遞迴深度
max_recursive_depth = 2

wikiArticle(root_keyword_link, input_keyword, 0)

